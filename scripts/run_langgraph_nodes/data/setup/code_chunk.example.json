{
  "chunk_id": "data_gathering_and_processing",
  "description": "Gathers and processes data from various sources, including grants and woodland information",
  "files": [
    "src/api/gather-data/controllers/post-gather-data.js",
    "src/api/gather-data/domain/processor.js",
    "src/api/gather-data/helpers/gather-data.js",
    "src/api/gather-data/index.js",
    "src/api/gather-data/services/azure-search-service.js",
    "src/api/gather-data/services/farming-finder.js",
    "src/api/gather-data/services/govuk-api.js",
    "src/api/gather-data/services/openai-service.js",
    "src/api/gather-data/services/s3-client.js",
    "src/api/gather-data/services/vet-visits.js",
    "src/api/gather-data/services/woodland-offer.js",
    "src/api/gather-data/services/woodland.js",
    "src/api/gather-data/utils/chunker.js",
    "src/api/gather-data/utils/markdown-utils.js"
  ],
  "content": "\n\n--- src/api/gather-data/controllers/post-gather-data.js ---\nimport { createLogger } from '~/src/helpers/logging/logger.js'\nimport { gatherData } from '~/src/api/gather-data/helpers/gather-data.js'\n\nconst postGatherData = {\n  /**\n   * @param { import('@hapi/hapi').Request } request\n   * @param { import('@hapi/hapi').ResponseToolkit } h\n   * @returns {Promise<*>}\n   */\n  handler: async (request, h) => {\n    const logger = createLogger()\n    logger.info(`Gather data started at ${new Date().toISOString()}`)\n\n    const results = await gatherData()\n\n    return h.response({ message: 'success', results }).code(200)\n  }\n}\n\nexport { postGatherData }\n\n\n--- src/api/gather-data/domain/processor.js ---\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nimport { SearchClient } from '@azure/search-documents'\nimport crypto from 'node:crypto'\n\nimport { chunkDocument } from '~/src/api/gather-data/utils/chunker.js'\nimport {\n  uploadDocument,\n  deleteDocuments\n} from '~/src/api/gather-data/services/azure-search-service.js'\nimport {\n  generateEmbedding,\n  generateShortSummary\n} from '~/src/api/gather-data/services/openai-service.js'\nimport {\n  getManifest,\n  uploadManifest\n} from '~/src/api/gather-data/services/s3-client.js'\nimport { createLogger } from '~/src/helpers/logging/logger.js'\n\n/**\n * @typedef {{link: string, lastModified: string, documentKeys: string[], summariesKeys: string[]}} Manifest\n */\n\n/**\n * Chunks and uploads grants to AI Search & removes old grants from AI Search\n * @param {{ grants: import('../services/govuk-api.js').Grant[], scheme: { manifestFile: string, schemeName: string }, searchClient: SearchClient, searchSummariesClient: SearchClient }} props\n * @returns {Promise<{ chunkCount: number, processedGrants: Manifest[] }>}\n */\nconst process = async ({\n  grants,\n  scheme,\n  searchClient,\n  searchSummariesClient\n}) => {\n  const manifestGrants = await getManifest(scheme.manifestFile)\n\n  const removedGrants = manifestGrants.filter((manifestGrant) =>\n    isGrantRemoved(manifestGrant, grants)\n  )\n  const removedGrantLinks = removedGrants.map((grant) => grant.link)\n  const manifestData = manifestGrants.filter(\n    (grant) => !removedGrantLinks.includes(grant.link)\n  )\n  await processRemovedGrants(removedGrants, searchClient, searchSummariesClient)\n\n  const result = await processGrants({\n    grants,\n    manifestGrants: manifestData,\n    schemeName: scheme.schemeName,\n    searchClient,\n    searchSummariesClient\n  })\n\n  manifestData.push(...result.processedGrants)\n  await uploadManifest(manifestData, scheme.manifestFile)\n\n  return result\n}\n\n/**\n * Chunks and uploads grants to AI Search\n * @param {{ grants: import('../services/govuk-api.js').Grant[], manifestGrants: Manifest[], schemeName: string, searchClient: SearchClient, searchSummariesClient: SearchClient, summaryTokenLimit?: number }} props\n * @returns {Promise<{ chunkCount: number, processedGrants: Manifest[] }>}\n */\nconst processGrants = async ({\n  grants,\n  manifestGrants,\n  schemeName,\n  searchClient,\n  searchSummariesClient,\n  summaryTokenLimit = 100\n}) => {\n  const processedGrants = []\n  let chunkCount = 0\n\n  for (const [index, grant] of grants.entries()) {\n    const logger = createLogger()\n    try {\n      if (isGrantOutofDate(grant, manifestGrants)) {\n        const sourceURL = grant.url.replace('/api/content', '')\n        const grantHash = crypto\n          .createHash('md5')\n          .update(grant.title)\n          .digest('hex')\n        const keys = []\n        logger.info(\n          `Processing grant ${index + 1}/${grants.length}... ${sourceURL}`\n        )\n\n        const chunks = chunkDocument({\n          document: grant.content,\n          title: grant.title,\n          grantSchemeName: schemeName,\n          sourceUrl: sourceURL\n        })\n\n        for (const [index, chunk] of chunks.entries()) {\n          logger.debug(`Processing chunk ${index + 1}/${chunks.length}...`)\n          const chunkHash = crypto.createHash('md5').update(chunk).digest('hex')\n          const embedding = await generateEmbedding(chunk)\n\n          const documentChunk = {\n            chunk_id: chunkHash,\n            parent_id: grantHash,\n            chunk,\n            title: grant.title,\n            grant_scheme_name: schemeName,\n            source_url: sourceURL,\n            content_vector: embedding\n          }\n\n          const processedKeys = await uploadDocument(\n            documentChunk,\n            searchClient\n          )\n          keys.push(...processedKeys)\n          chunkCount++\n        }\n\n        const shortSummary = await generateShortSummary(\n          grant.content,\n          summaryTokenLimit\n        )\n\n        const summariesChunks = chunkDocument({\n          document: shortSummary,\n          title: grant.title,\n          grantSchemeName: schemeName,\n          sourceUrl: sourceURL\n        })\n\n        const summariesKeys = []\n        // given the summary is short, there should only be one chunk\n        for (const [index, chunk] of summariesChunks.entries()) {\n          logger.debug(\n            `Processing summary chunk ${index + 1}/${summariesChunks.length}...`\n          )\n          const chunkHash = crypto.createHash('md5').update(chunk).digest('hex')\n          const embedding = await generateEmbedding(chunk)\n\n          const summaryChunk = {\n            chunk_id: chunkHash,\n            parent_id: grantHash,\n            chunk,\n            title: grant.title,\n            grant_scheme_name: schemeName,\n            source_url: sourceURL,\n            content_vector: embedding\n          }\n\n          // Upload short summary\n          const processedKeys = await uploadDocument(\n            summaryChunk,\n            searchSummariesClient\n          )\n          summariesKeys.push(...processedKeys)\n        }\n\n        processedGrants.push({\n          link: grant.url,\n          lastModified: grant.updateDate.toISOString(),\n          documentKeys: keys,\n          summariesKeys\n        })\n      }\n    } catch (error) {\n      logger.error(error, 'Error uploading grant')\n    }\n  }\n\n  return {\n    chunkCount,\n    processedGrants\n  }\n}\n\n/**\n * Removes out of date grant documents from AI search\n * @param {Manifest[]} removedGrants\n * @param {SearchClient} searchClient\n * @param {SearchClient} searchSummariesClient\n * @returns boolean\n */\nconst processRemovedGrants = async (\n  removedGrants,\n  searchClient,\n  searchSummariesClient\n) => {\n  if (removedGrants.length === 0) {\n    return true\n  }\n\n  const keys = removedGrants.flatMap(\n    (removedGrant) => removedGrant.documentKeys\n  )\n  const summaryKeys = removedGrants.flatMap(\n    (removedGrant) => removedGrant.summariesKeys\n  )\n\n  const result = await deleteDocuments(keys, searchClient)\n  const summaryResult = await deleteDocuments(\n    summaryKeys,\n    searchSummariesClient\n  )\n\n  return result && summaryResult\n}\n\n/**\n * Returns true if grant s\n * @param {Manifest} manifestGrant\n * @param {import('../services/govuk-api.js').Grant[]} grants\n * @returns boolean\n */\nconst isGrantRemoved = (manifestGrant, grants) => {\n  const matchedGrants = grants.filter(\n    (grant) => grant.url === manifestGrant.link\n  )\n\n  return matchedGrants.length === 0\n}\n\n/**\n * Returns true if a live grant's update date is newer than the manifest's update date\n * @param {import('../services/govuk-api.js').Grant} grant\n * @param {Manifest[]} manifestGrants\n * @returns boolean\n */\nconst isGrantOutofDate = (grant, manifestGrants) => {\n  // if the grant's last updated date is newer than manifest, add in the grant\n  // if this link is not present in the manifest, also add in the grant\n  const manifest = manifestGrants.find(\n    (manifest) => manifest.link === grant.url\n  )\n  if (!manifest?.lastModified) {\n    return true\n  }\n  const manifestDate = new Date(manifest.lastModified)\n  return grant.updateDate > manifestDate\n}\nexport { process }\n\n\n--- src/api/gather-data/helpers/gather-data.js ---\nimport { createLogger } from '~/src/helpers/logging/logger.js'\nimport { config } from '~/src/config/index.js'\nimport {\n  getFinderGrants,\n  getNumberOfGrants\n} from '~/src/api/gather-data/services/farming-finder.js'\nimport { getVetVisits } from '~/src/api/gather-data/services/vet-visits.js'\nimport { getWoodlandGrants } from '~/src/api/gather-data/services/woodland.js'\nimport { getWoodlandOfferGrants } from '~/src/api/gather-data/services/woodland-offer.js'\nimport { process } from '~/src/api/gather-data/domain/processor.js'\nimport {\n  getSearchClient,\n  getSearchSummariesClient\n} from '~/src/api/gather-data/services/azure-search-service.js'\n\nexport async function gatherData() {\n  const logger = createLogger()\n  const searchClient = getSearchClient()\n  const searchSummariesClient = getSearchSummariesClient()\n\n  const totalFarmingFinderGrants = await getNumberOfGrants()\n\n  const responseSfi = await processFarmingFinderData({\n    searchClient,\n    count: totalFarmingFinderGrants,\n    searchSummariesClient\n  })\n\n  const responseWoodland = await processWoodlandData({\n    searchClient,\n    searchSummariesClient\n  })\n  const responseVetVisits = await processVetVisitsData({\n    searchClient,\n    searchSummariesClient\n  })\n  const responseWoodlandOffer = await processWoodlandOfferData({\n    searchClient,\n    searchSummariesClient\n  })\n\n  logger.info(`Finished running gather data at ${new Date().toISOString()}`)\n\n  const results = {\n    farmingFinder: {\n      addedGrants: responseSfi.processedGrantsCount\n    },\n    woodlandCreationPartnership: {\n      addedGrants: responseWoodland.processedGrantsCount\n    },\n    vetVisits: {\n      addedGrants: responseVetVisits.processedGrantsCount\n    },\n    woodlandOffer: {\n      addedGrants: responseWoodlandOffer.processedGrantsCount\n    }\n  }\n\n  return results\n}\n\nconst processVetVisitsData = async ({\n  searchClient,\n  searchSummariesClient\n}) => {\n  const scheme = config.get('vetVisits')\n  const grants = await getVetVisits()\n\n  const result = await processGrants({\n    grants,\n    scheme,\n    searchClient,\n    searchSummariesClient\n  })\n\n  return result\n}\n\nconst processWoodlandData = async ({ searchClient, searchSummariesClient }) => {\n  const scheme = config.get('woodlandCreation')\n  const grants = await getWoodlandGrants()\n\n  const result = await processGrants({\n    grants,\n    scheme,\n    searchClient,\n    searchSummariesClient\n  })\n\n  return result\n}\n\nconst processWoodlandOfferData = async ({\n  searchClient,\n  searchSummariesClient\n}) => {\n  const scheme = config.get('woodlandOffer')\n  const grants = await getWoodlandOfferGrants()\n\n  const result = await processGrants({\n    grants,\n    scheme,\n    searchClient,\n    searchSummariesClient\n  })\n\n  return result\n}\n\nconst processFarmingFinderData = async ({\n  searchClient,\n  count,\n  searchSummariesClient\n}) => {\n  const scheme = config.get('farmingFinder')\n  const grants = await getFinderGrants(count)\n\n  const result = await processGrants({\n    grants,\n    scheme,\n    searchClient,\n    searchSummariesClient\n  })\n\n  return result\n}\n\nconst processGrants = async ({\n  grants,\n  scheme,\n  searchClient,\n  searchSummariesClient\n}) => {\n  try {\n    const { chunkCount, processedGrants } = await process({\n      grants,\n      scheme,\n      searchClient,\n      searchSummariesClient\n    })\n\n    return {\n      chunkCount,\n      processedGrantsCount: processedGrants.length\n    }\n  } catch (error) {\n    const logger = createLogger()\n    logger.error(error)\n\n    return {\n      processedGrantsCount: 0\n    }\n  }\n}\n\n\n--- src/api/gather-data/index.js ---\nimport { postGatherData } from '~/src/api/gather-data/controllers/post-gather-data.js'\n\n/**\n * @satisfies {ServerRegisterPluginObject<void>}\n */\nconst gatherData = {\n  plugin: {\n    name: 'gather-data',\n    register: (server) => {\n      server.route([\n        {\n          method: 'POST',\n          path: '/gather-data',\n          ...postGatherData\n        }\n      ])\n    }\n  }\n}\n\nexport { gatherData }\n\n/**\n * @import { ServerRegisterPluginObject } from '@hapi/hapi'\n */\n\n\n--- src/api/gather-data/services/azure-search-service.js ---\nimport { SearchClient, AzureKeyCredential } from '@azure/search-documents'\n\nimport { config } from '~/src/config/index.js'\nimport { createLogger } from '~/src/helpers/logging/logger.js'\n\n/**\n * Upload document to Azure AI Search\n * @param {{ chunk_id: string, parent_id: string, chunk: string, title: string, grant_scheme_name: string, source_url: string, content_vector: number[] }} document\n * @param { SearchClient } searchClient\n * @returns { Promise<string[]> }\n */\nconst uploadDocument = async (document, searchClient) => {\n  const logger = createLogger()\n  const uploadResult = await searchClient.uploadDocuments([document])\n\n  const uploadedKeys = uploadResult.results\n    .filter((result) => result.succeeded)\n    .map((result) => result.key)\n\n  const failedKeys = uploadResult.results\n    .filter((result) => !result.succeeded)\n    .map((result) => result.key)\n\n  if (failedKeys.length > 0) {\n    logger.info('failed keys: ', failedKeys)\n  }\n\n  return uploadedKeys\n}\n\n/**\n * Delete documents from Azure AI Search\n * @param {string[]} keys\n * @param {SearchClient} searchClient\n * @returns boolean\n */\nconst deleteDocuments = async (keys, searchClient) => {\n  const deletedRes = await searchClient.deleteDocuments(\n    config.get('azureOpenAI.primaryKeyName'),\n    keys\n  )\n  const unsuccessfulDeletes = deletedRes.results.filter(\n    (result) => !result.succeeded\n  )\n\n  return unsuccessfulDeletes.length === 0\n}\n\nconst getSearchClient = () => {\n  const { searchUrl, indexName, searchApiKey } = config.get('azureOpenAI')\n  const proxyUrlConfig = config.get('httpsProxy') ?? config.get('httpProxy')\n  let proxyOptions\n  if (proxyUrlConfig) {\n    const proxyUrl = new URL(proxyUrlConfig)\n    const port = proxyUrl.protocol.toLowerCase() === 'http:' ? 80 : 443\n    proxyOptions = {\n      host: proxyUrl.href,\n      port\n    }\n  }\n\n  const searchClient = new SearchClient(\n    searchUrl,\n    indexName,\n    new AzureKeyCredential(searchApiKey),\n    {\n      proxyOptions\n    }\n  )\n\n  return searchClient\n}\n\nconst getSearchSummariesClient = () => {\n  const { searchUrl, summaryIndexName, searchApiKey } =\n    config.get('azureOpenAI')\n  const proxyUrlConfig = config.get('httpsProxy') ?? config.get('httpProxy')\n  let proxyOptions\n  if (proxyUrlConfig) {\n    const proxyUrl = new URL(proxyUrlConfig)\n    const port = proxyUrl.protocol.toLowerCase() === 'http:' ? 80 : 443\n    proxyOptions = {\n      host: proxyUrl.href,\n      port\n    }\n  }\n\n  const searchClient = new SearchClient(\n    searchUrl,\n    summaryIndexName,\n    new AzureKeyCredential(searchApiKey),\n    {\n      proxyOptions\n    }\n  )\n\n  return searchClient\n}\n\nexport {\n  getSearchClient,\n  getSearchSummariesClient,\n  uploadDocument,\n  deleteDocuments\n}\n\n\n--- src/api/gather-data/services/farming-finder.js ---\n/* eslint-disable @typescript-eslint/ban-ts-comment */\n// @ts-nocheck\nimport { config } from '~/src/config/index.js'\nimport { createLogger } from '~/src/helpers/logging/logger.js'\nimport { getGovukContent } from '~/src/api/gather-data/services/govuk-api.js'\nimport { proxyFetch } from '~/src/helpers/proxy-fetch.js'\n\n/**\n * Get grants from \"Find funding for land or farms\" finder\n * @param {number} count\n * @returns {Promise<import('./govuk-api.js').Grant[]>}\n */\nasync function getFinderGrants(count) {\n  const urls = await getLinksFromSearchApi(count)\n  const grants = []\n\n  for (const url of urls) {\n    try {\n      const grant = await getGovukContent(url)\n\n      grants.push(grant)\n    } catch (error) {\n      const logger = createLogger()\n      logger.error(error)\n    }\n  }\n\n  return grants\n}\n\n/**\n * Get \"link\" values from search API from Specialist Publisher tool\n * @param {number} count\n * @returns {Promise<string[]>}\n */\nasync function getLinksFromSearchApi(count) {\n  const url = `${config.get('farmingFinder.searchUrl')}&count=${count}`\n  const response = await proxyFetch(url, {})\n\n  const json = await response.json()\n\n  const links = json.results.map(\n    (result) => config.get('farmingFinder.findFarmingUrl') + result.link\n  )\n\n  return links\n}\n\n/**\n * Get the total number of grants in farming funding finder\n * @returns {Promise<number>}\n */\nasync function getNumberOfGrants() {\n  const logger = createLogger()\n  const url = `${config.get('farmingFinder.searchUrl')}`\n  const response = await proxyFetch(url, {})\n  const json = await response.json()\n\n  logger.info(`Fetch ${url}. Status Code ${response.status}`)\n\n  return json.total\n}\n\nexport { getFinderGrants, getNumberOfGrants }\n\n\n--- src/api/gather-data/services/govuk-api.js ---\n/* eslint-disable @typescript-eslint/ban-ts-comment */\n// @ts-nocheck\nimport { proxyFetch } from '~/src/helpers/proxy-fetch.js'\n\nimport {\n  webpageToMarkdown,\n  stripLinksFromMarkdown\n} from '~/src/api/gather-data/utils/markdown-utils.js'\n\n/**\n * @typedef {{title: string, content: string, updateDate: Date, url: string}} Grant\n */\n\n/**\n * Fetches content from GOV.UK\n * @param {string} url - GOV.UK API URL (Needs to include /api/content in the URL)\n * @returns {Promise<Grant>}\n */\nconst getGovukContent = async (url) => {\n  const response = await proxyFetch(url, {})\n  const responseJson = await response.json()\n\n  if (\n    response.status !== 200 ||\n    !responseJson.title ||\n    // eslint-disable-next-line @typescript-eslint/prefer-optional-chain\n    !responseJson.details ||\n    !responseJson.details.body\n  ) {\n    throw new Error(`Unable to parse content from ${url}`)\n  }\n\n  let title = responseJson.title\n  const updateDate = new Date(responseJson.updated_at)\n\n  title = title.replace('\\\\', '')\n  title = title.replace('/', 'or')\n\n  const responseBody = responseJson.details.body\n\n  const markdownContent = webpageToMarkdown(responseBody)\n  const strippedContent = stripLinksFromMarkdown(markdownContent)\n\n  return {\n    title,\n    content: strippedContent,\n    updateDate,\n    url\n  }\n}\n\nexport { getGovukContent }\n\n\n--- src/api/gather-data/services/openai-service.js ---\nimport { OpenAIEmbeddings, ChatOpenAI } from '@langchain/openai'\nimport { URL } from 'node:url'\nimport { HttpsProxyAgent } from 'https-proxy-agent'\n\nimport { config } from '~/src/config/index.js'\nimport { createLogger } from '~/src/helpers/logging/logger.js'\n\n// eslint-disable-next-line @typescript-eslint/require-await\nconst onFailedAttempt = async (error) => {\n  if (error.retriesLeft === 0) {\n    const logger = createLogger()\n    logger.error(error, 'Failed to get embeddings')\n    throw error\n  }\n}\n\n/**\n * Generates vector embeddings for content\n * @param {string} chunk\n * @returns {Promise<number[]>}\n */\nconst generateEmbedding = async (chunk) => {\n  const proxyUrlConfig = config.get('httpsProxy') ?? config.get('httpProxy')\n  let httpAgent\n  if (proxyUrlConfig) {\n    const proxyUrl = new URL(proxyUrlConfig)\n    httpAgent = new HttpsProxyAgent(proxyUrl)\n  }\n\n  const embeddings = new OpenAIEmbeddings({\n    azureOpenAIApiInstanceName: config.get('azureOpenAI.openAiInstanceName'),\n    azureOpenAIApiKey: config.get('azureOpenAI.openAiKey'),\n    azureOpenAIApiDeploymentName: 'text-embedding-ada-002',\n    azureOpenAIApiVersion: '2024-02-01',\n    configuration: {\n      httpAgent\n    },\n    onFailedAttempt\n  })\n\n  const embedding = await embeddings.embedDocuments([chunk])\n\n  return embedding[0]\n}\n\n/**\n * Generates a short summary for a given text\n * @param {string} text - The text to summarize\n * @param {number | undefined} summaryTokenLimit - The token limit for the summary\n * @returns {Promise<string>} - The generated summary\n */\nconst generateShortSummary = async (text, summaryTokenLimit = 100) => {\n  const proxyUrlConfig = config.get('httpsProxy') ?? config.get('httpProxy')\n  let httpAgent\n  if (proxyUrlConfig) {\n    const proxyUrl = new URL(proxyUrlConfig)\n    httpAgent = new HttpsProxyAgent(proxyUrl)\n  }\n\n  const model = new ChatOpenAI({\n    azureOpenAIApiInstanceName: config.get('azureOpenAI.openAiInstanceName'),\n    azureOpenAIApiKey: config.get('azureOpenAI.openAiKey'),\n    azureOpenAIApiDeploymentName: 'gpt-35-turbo-16k',\n    azureOpenAIApiVersion: '2024-02-01',\n    configuration: {\n      httpAgent\n    },\n    onFailedAttempt\n  })\n\n  const messages = [\n    [\n      'user',\n      `\n      Generate a summary of the following text without explaining that this is a summary. The result should be in the same tone and style as the original text. Limit the summary to ${summaryTokenLimit} tokens:\n      ${text}\n    `\n    ]\n  ]\n\n  try {\n    const response = await model.generate(messages)\n\n    return response.generations.flat()[0].text.replace(/\\n/g, ' ').trim()\n  } catch (error) {\n    const logger = createLogger()\n    try {\n      logger.error(error, 'Error generating summary using openai')\n      if (typeof document !== 'string') {\n        logger.error('Document is not a string', document)\n        return ''\n      }\n\n      // eslint-disable-next-line @typescript-eslint/ban-ts-comment\n      // @ts-ignore\n      const summary = document.split(' ').slice(0, summaryTokenLimit).join(' ')\n      const lastSentenceEnd = Math.max(\n        summary.lastIndexOf('.'),\n        summary.lastIndexOf('\\n')\n      )\n\n      return summary.slice(0, lastSentenceEnd + 1)\n    } catch (error) {\n      logger.error(error, 'Error generating summary using fallback')\n      return ''\n    }\n  }\n}\n\nexport { generateEmbedding, generateShortSummary }\n\n\n--- src/api/gather-data/services/s3-client.js ---\nimport {\n  S3Client,\n  GetObjectCommand,\n  PutObjectCommand\n} from '@aws-sdk/client-s3'\n\nimport { config } from '~/src/config/index.js'\nimport { createLogger } from '~/src/helpers/logging/logger.js'\n\n/**\n * Upload manifest to s3 bucket\n * @param {import('../domain/processor.js').Manifest[]} manifestData\n * @param {string} manifestFilename\n */\nasync function uploadManifest(manifestData, manifestFilename) {\n  const logger = createLogger()\n  try {\n    const s3Client = getS3Client()\n\n    const manifestText = JSON.stringify(manifestData, null, 2)\n\n    await s3Client.send(\n      new PutObjectCommand({\n        Bucket: config.get('aws.s3bucket'),\n        Body: manifestText,\n        Key: manifestFilename\n      })\n    )\n\n    logger.info(`Manifest uploaded successfully: ${manifestFilename}`)\n  } catch (error) {\n    logger.error(error, 'Error uploading manifest to S3')\n  }\n}\n\n/**\n * Get a list of grant links processed in previous runs\n * @param {string} manifestFilename\n * @returns {Promise<import('../domain/processor.js').Manifest[]>}\n */\nasync function getManifest(manifestFilename) {\n  const logger = createLogger()\n\n  try {\n    const s3Client = getS3Client()\n\n    const obj = await s3Client.send(\n      new GetObjectCommand({\n        Bucket: config.get('aws.s3bucket'),\n        Key: manifestFilename\n      })\n    )\n\n    const body = await obj.Body?.transformToString()\n\n    const manifestJSON = JSON.parse(body ?? '[]')\n\n    return manifestJSON\n  } catch (error) {\n    if (!error.statusCode || error.statusCode !== 404) {\n      logger.error(error, 'Error fetching Manifest')\n    }\n\n    return []\n  }\n}\n\nfunction getS3Client() {\n  return new S3Client({\n    region: config.get('aws.region'),\n    ...(config.get('aws.s3Endpoint') && {\n      endpoint: config.get('aws.s3Endpoint'),\n      forcePathStyle: true\n    })\n  })\n}\n\nexport { getManifest, uploadManifest }\n\n\n--- src/api/gather-data/services/vet-visits.js ---\nimport { config } from '~/src/config/index.js'\nimport { getGovukContent } from '~/src/api/gather-data/services/govuk-api.js'\n\nasync function getVetVisits() {\n  const grant = await getGovukContent(config.get('vetVisits.url'))\n\n  return [grant]\n}\n\nexport { getVetVisits }\n\n\n--- src/api/gather-data/services/woodland-offer.js ---\nimport { config } from '~/src/config/index.js'\nimport { getGovukContent } from '~/src/api/gather-data/services/govuk-api.js'\n\nasync function getWoodlandOfferGrants() {\n  const grant = await getGovukContent(config.get('woodlandOffer.url'))\n\n  return [grant]\n}\n\nexport { getWoodlandOfferGrants }\n\n\n--- src/api/gather-data/services/woodland.js ---\n/* eslint-disable @typescript-eslint/ban-ts-comment */\n// @ts-nocheck\nimport * as cheerio from 'cheerio'\nimport { proxyFetch } from '~/src/helpers/proxy-fetch.js'\n\nimport { config } from '~/src/config/index.js'\nimport {\n  webpageToMarkdown,\n  stripLinksFromMarkdown\n} from '~/src/api/gather-data/utils/markdown-utils.js'\n\n/**\n * Get woodland grants\n * @returns {Promise<{title: string, content: string, updateDate: Date, url: string}[]>}\n */\nasync function getWoodlandGrants() {\n  const url = config.get('woodlandCreation.url')\n  const response = await proxyFetch(url, {})\n  const responseJSON = await response.json()\n\n  const updateDate = new Date(responseJSON.updated_at)\n\n  const responseBody = responseJSON.details.body\n\n  const grants = splitWoodlandContentBody(responseBody)\n\n  const markdownGrants = []\n\n  for (const grant of grants) {\n    const markdown = webpageToMarkdown(grant.content)\n    const strippedContent = stripLinksFromMarkdown(markdown)\n\n    markdownGrants.push({\n      title: grant.title,\n      content: strippedContent,\n      updateDate,\n      url\n    })\n  }\n\n  return markdownGrants\n}\n\n/**\n * Remove ignored grants and return relevant grants as an array\n * @param {string} body\n * @returns {{title: string, content: string}[]}\n */\nfunction splitWoodlandContentBody(body) {\n  const $ = cheerio.load(body)\n  const sections = []\n  let currentSection = { title: '', content: '' }\n\n  $('div')\n    .children()\n    .each((index, element) => {\n      if (element.tagName === 'h2') {\n        if (currentSection.title || currentSection.content) {\n          sections.push(currentSection)\n          currentSection = { title: '', content: '' }\n        }\n        currentSection.title = $(element).text()\n      } else {\n        currentSection.content += $.html(element)\n      }\n    })\n\n  if (currentSection.title || currentSection.content) {\n    sections.push(currentSection)\n  }\n  // Remove first 5 elements as these are either irrelevant or closed grants that we don't want to process\n  return sections.slice(5)\n}\n\nexport { getWoodlandGrants }\n\n\n--- src/api/gather-data/utils/chunker.js ---\nimport * as tiktoken from 'js-tiktoken'\n\n/**\n * Chunk a long string into multiple smaller parts\n * @param {{document: string, title: string, grantSchemeName: string, sourceUrl: string, tokenLimit?: number}} chunkProps\n * @returns string[]\n */\nfunction chunkDocument({\n  document,\n  title,\n  grantSchemeName,\n  sourceUrl,\n  tokenLimit = 512\n}) {\n  const encoding = tiktoken.encodingForModel('gpt-3.5-turbo-16k-0613')\n  const chunks = []\n  let tokens = encoding.encode(document, [], [])\n\n  let chunkNumber = 1\n  const baseIdentifier = `(Title: ${title} | Grant Scheme Name: ${grantSchemeName} | Source: ${sourceUrl} | Chunk Number: `\n\n  while (tokens.length > 0) {\n    const identifier = `${baseIdentifier}${chunkNumber})===`\n    const identifierTokens = encoding.encode(identifier)\n    const availableTokenLimit = tokenLimit - identifierTokens.length\n\n    const chunk = tokens.slice(0, availableTokenLimit)\n    let chunkText = encoding.decode(chunk)\n\n    const lastSentenceEnd = Math.max(\n      chunkText.lastIndexOf('.'),\n      chunkText.lastIndexOf('\\n')\n    )\n\n    if (lastSentenceEnd !== -1 && tokens.length > availableTokenLimit) {\n      chunkText = chunkText.slice(0, lastSentenceEnd + 1)\n    }\n\n    const cleanedText = chunkText.replace(/\\n/g, ' ').trim()\n\n    if (cleanedText && !/^\\s*$/.test(cleanedText)) {\n      chunks.push(`${identifier}${cleanedText}`)\n      chunkNumber++\n    }\n\n    // Move the start of the next chunk to the end of the last sentence\n    const overlapStart = lastSentenceEnd + 1\n    tokens = tokens.slice(\n      overlapStart\n        ? encoding.encode(chunkText.slice(0, overlapStart)).length\n        : availableTokenLimit\n    )\n  }\n\n  return chunks\n}\n\nexport { chunkDocument }\n\n\n--- src/api/gather-data/utils/markdown-utils.js ---\nimport Turndown from 'turndown'\n\nfunction webpageToMarkdown(body) {\n  if (!body) return undefined\n\n  // Convert HTML string to Markdown string using turndown\n  const turndownService = new Turndown({ headingStyle: 'atx' })\n  const markdownString = turndownService.turndown(body)\n\n  return markdownString\n}\n\nfunction stripLinksFromMarkdown(markdownContent) {\n  const linkPattern = /\\[(.*?)\\]\\((.*?)\\)/g\n\n  return markdownContent.replace(linkPattern, (_, linkText) => {\n    return linkText\n  })\n}\n\nexport { webpageToMarkdown, stripLinksFromMarkdown }\n"
}
